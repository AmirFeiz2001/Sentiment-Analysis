# -*- coding: utf-8 -*-
"""sentiment.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_TK2xBOgmp5dy1aOHaHR3ZPBwaRvRp8T
"""
# install Hazm
!pip install hazm
#=======================================================
# import required libraries
from hazm import *
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from collections import Counter
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding , LSTM , Dense , Dropout
from keras.initializers import Constant
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.layers import BatchNormalization
#====================================================
# load csv file and create dataframe
def load_data(file_name):
    f = open(file_name, 'r')
    data = pd.read_csv(f, delimiter=',', encoding='utf-8')
    return data

file_name = os.path.join('datacompet.csv')
df = load_data(file_name)
df.head()
#===================================================
# prepare dataframe and label the news
train = df.rename(columns={'            انکار دستاوردهای دولت انکار دستاوردهای نظام است': 'text'})

train['target'] = ['0','1','0','1','0','1','1','0','1','1','0','0','0','1','1','1','1','0','1','0','1','1','1','1','1','1','1','1','1','1','0','1','1','1','0','1','0','1','1','1','1','0','1','0','0','1','1','0','1','0','0','1','0','0','1','1','1','1','1','0','0','1','1','1','1','1','1','1','1','1','1','1','1','0','0','1','1','1','1','0','0','1','1','0','1','1','1','1','1','1','1','1','1','0','1','0','1','1','0','1','1']

label = []
for i in train_df['target']:
  label.append(int(i))

train['target'] = label
#==================================================
# prepare data for training

# count the unique words
def counter_word(text):
  count = Counter()
  for i in text:
    for word in i.split():
      count[word] +=1
  return count

text = train.text
counter = counter_word(text)

num_words = len(counter)
# max number of words in sequence
max_length = 20

# split data to train and test data
train_size = int(train.shape[0]*0.8)
train_sentences = train.text[:train_size]
train_labels = train.target[:train_size]
test_sentences = train.text[train_size:]
test_labels = train.target[train_size:]

# import Tokenizer class from keras to tokenize the train sentences
tokenizer = Tokenizer(num_words=num_words)
tokenizer.fit_on_texts(train_sentences)

# get the index of each tokenized words
word_index = tokenizer.word_index

# create the sequences from our tokenizer based on the indexes from the word_index
train_sequences = tokenizer.texts_to_sequences(train_sentences)

# we have to pad the sequences to have the same length for each sequence
# because when we use LSTM we need to have the sequences of the same length
train_padded = pad_sequences(train_sequences,maxlen=max_length,padding='post',truncating='post')

# do the same for the test data
test_sequences = tokenizer.texts_to_sequences(test_sentences)
test_padded = pad_sequences(test_sequences,maxlen=max_length,padding='post',truncating='post')

# we want to find that the indexes shows the sentence correctly
reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])

def decode(text):
  return " ".join([reverse_word_index.get(i, "?") for i in text])

decode(train_sequences[0])

# the shape of the train and the test
print(f"shape of train {train_padded.shape}")
print(f"shape of test {test_padded.shape}")
#===============================================================
# create model
model = Sequential()
model.add(Embedding(num_words, 32, input_length=max_length))
model.add(LSTM(64,dropout=0.1))
model.add(Dense(1, activation='sigmoid'))
#==============================================================
# compile the model
optimizer = Adam(learning_rate=3e-8)
model.compile(loss='binary_crossentropy', optimizer=optimizer,metrics=['accuracy'])
#=============================================================
# see a summary of model
model.summary()
#============================================================
# train model
earlystop = EarlyStopping(patience=4)
history = model.fit(train_padded,train_labels,epochs=20, validation_data=(test_padded,test_labels),callbacks=[earlystop])
#=============================================================
# prediction
def predict(train_padded,model):
  if ((model.predict(test)).tolist()[0][0])> 0.5:
    print('positive sentiment')

  else:
    print('negetive sentiment')

def usable(text):
  counter = counter_word(text)
  num_words = len(counter)
  max_length = 20
  tokenizer = Tokenizer(num_words=num_words)
  tokenizer.fit_on_texts(text)
  word_index = tokenizer.word_index
  train_sequences = tokenizer.texts_to_sequences(text)
  train_padded = pad_sequences(train_sequences,maxlen=max_length,padding='post',truncating='post')
  return train_padded

print('Enter number of the news you want to add:')
number = int(input())
print("Please enter your news:")
text = []
for j in range(number):
  text.append(input())
for i in text:
  test = usable(i)
  prediction = predict(test,model)
  print('------------------------------------------------')

